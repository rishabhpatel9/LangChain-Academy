{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717edf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db64c0fd-9355-49e4-8290-2f2ae08eeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "\n",
    "# Fix for Windows issues in Jupyter notebooks\n",
    "if sys.platform == \"win32\":\n",
    "    # 1. Use ProactorEventLoop for subprocess support\n",
    "    if not isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    \n",
    "    # 2. Redirect stderr to avoid fileno() error when launching MCP servers\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        sys.stderr = sys.__stderr__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d701224",
   "metadata": {},
   "source": [
    "## Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f11678d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\",\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"resources/2.1_mcp_server.py\"],\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "184db1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d548fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "agent = create_agent(\n",
    "    model=ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\"),\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5256ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3efb5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Tell me about the langchain-mcp-adapters library', additional_kwargs={}, response_metadata={}, id='eb3da245-9ea8-4dbf-b104-19571c77aa76'),\n",
      "              AIMessage(content='', additional_kwargs={'function_call': {'name': 'search_web', 'arguments': '{\"query\": \"langchain-mcp-adapters library\"}'}}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019ca373-6932-7902-ba7e-eb0577605770-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters library'}, 'id': '8d910557-8b3d-44d6-b427-b6be9ae876e7', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 194, 'output_tokens': 22, 'total_tokens': 216, 'input_token_details': {'cache_read': 0}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"query\": \"langchain-mcp-adapters library\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchainjs-mcp-adapters\",\\n      \"title\": \"GitHub - langchain-ai/langchainjs-mcp-adapters: ** THIS ...\",\\n      \"content\": \"# Search code, repositories, users, issues, pull requests... You signed in with another tab or window. You signed out in another tab or window. langchain-ai   /  **langchainjs-mcp-adapters**  Public archive. \\\\\\\\*\\\\\\\\* THIS REPO HAS MOVED TO  \\\\\\\\*\\\\\\\\* Adapters for integrating Model Context Protocol (MCP) tools with LangChain.js applications, supporting both stdio and SSE transports. 246 stars   34 forks   Branches   Tags   Activity. # langchain-ai/langchainjs-mcp-adapters. | Name | Name | Last commit message | Last commit date |. | tsconfig.cjs.json | tsconfig.cjs.json |  |  |. | tsconfig.examples.json | tsconfig.examples.json |  |  |. | tsconfig.tests.json | tsconfig.tests.json |  |  |. ## Repository files navigation. # LangChain.js MCP Adapters. This library provides a lightweight wrapper to allow Model Context Protocol (MCP) services to be used with LangChain.js. \\\\\\\\*\\\\\\\\* THIS REPO HAS MOVED TO  \\\\\\\\*\\\\\\\\* Adapters for integrating Model Context Protocol (MCP) tools with LangChain.js applications, supporting both stdio and SSE transports. javascript   typescript   mcp   ai-tools   langchain   llm-tools   openai-functions   langchainjs   llm-agents   agent-tools   llm-integration   model-context-protocol.\",\\n      \"score\": 0.88705814,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.86981434,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://www.npmjs.com/package/@langchain/mcp-adapters\",\\n      \"title\": \"langchain/mcp-adapters\",\\n      \"content\": \"The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances. // Whether to throw on errors if a tool fails to load (optional, default: true). // Whether to prefix tool names with the server name (optional, default: false). // Whether to throw errors if a tool fails to load (optional, default: true). When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:. Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config. For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. const tools = await client.getTools(); // Only tools from \\\\\"working-server\\\\\".\",\\n      \"score\": 0.8593928,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://pypi.org/project/langchain-mcp-adapters/\",\\n      \"title\": \"langchain-mcp-adapters\",\\n      \"content\": \"Make Anthropic Model Context Protocol (MCP) tools compatible with LangChain and LangGraph agents. * ðŸ› ï¸ Convert MCP tools into LangChain tools that can be used with LangGraph agents. * ðŸ“¦ A client implementation that allows you to connect to multiple MCP servers and load tools from them. Here is a simple example of using the MCP tools with a LangGraph agent. from langchain_mcp_adapters.tools import load_mcp_tools. The library also allows you to connect to multiple MCP servers and load tools from them:. from langchain_mcp_adapters.client import MultiServerMCPClient. > from langchain_mcp_adapters.tools import load_mcp_tools. from langchain_mcp_adapters.tools import load_mcp_tools. from langchain_mcp_adapters.client import MultiServerMCPClient. from langchain_mcp_adapters.client import MultiServerMCPClient. These headers are passed with every HTTP request to the MCP server. from langchain_mcp_adapters.client import MultiServerMCPClient. If you want to run a LangGraph agent that uses MCP tools in a LangGraph API server, you can use the following setup:. from langchain_mcp_adapters.client import MultiServerMCPClient. Hashes for langchain\\\\\\\\_mcp\\\\\\\\_adapters-0.2.1-py3-none-any.whl. Details for the file `langchain_mcp_adapters-0.2.1.tar.gz`. Details for the file `langchain_mcp_adapters-0.2.1-py3-none-any.whl`.\",\\n      \"score\": 0.8497846,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/javascript/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP) - Docs by LangChain\",\\n      \"content\": \"Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the `@langchain/mcp-adapters` library. import { MultiServerMCPClient } from \\\\\"@langchain/mcp-adapters\\\\\"; import { MultiServerMCPClient } from \\\\\"@langchain/mcp-adapters\\\\\"; import { ChatAnthropic } from \\\\\"@langchain/anthropic\\\\\"; import { ChatAnthropic } from \\\\\"@langchain/anthropic\\\\\";import { createAgent } from \\\\\"langchain\\\\\"; import { createAgent } from  \\\\\"langchain\\\\\"; const client = new MultiServerMCPClient({ const  client =  new  MultiServerMCPClient({  math: { math: { transport: \\\\\"stdio\\\\\", // Local subprocess communication transport:  \\\\\"stdio\\\\\", // Local subprocess communication command: \\\\\"node\\\\\", command:  \\\\\"node\\\\\", // Replace with absolute path to your math_server.js file // Replace with absolute path to your math_server.js file args: [\\\\\"/path/to/math_server.js\\\\\"], args: [\\\\\"/path/to/math_server.js\\\\\"], }, }, weather: { weather: { transport: \\\\\"http\\\\\", // HTTP-based remote server transport:  \\\\\"http\\\\\", // HTTP-based remote server // Ensure you start your weather server on port 8000 // Ensure you start your weather server on port 8000 url: \\\\\"http://localhost:8000/mcp\\\\\", url: \\\\\"http://localhost:8000/mcp\\\\\", }, },});}); const tools = await client.getTools(); const  tools =  await  client.\",\\n      \"score\": 0.8479807,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 1.36,\\n  \"request_id\": \"77138cba-5c86-4808-b4db-71f061b653f5\"\\n}', 'id': 'lc_91ee6cc8-248e-4399-9a53-a6e696d36b22'}], name='search_web', id='d7a7d1f2-86d8-4a09-9831-271e6f9932d2', tool_call_id='8d910557-8b3d-44d6-b427-b6be9ae876e7', artifact={'structured_content': {'result': {'query': 'langchain-mcp-adapters library', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://github.com/langchain-ai/langchainjs-mcp-adapters', 'title': 'GitHub - langchain-ai/langchainjs-mcp-adapters: ** THIS ...', 'content': '# Search code, repositories, users, issues, pull requests... You signed in with another tab or window. You signed out in another tab or window. langchain-ai   /  **langchainjs-mcp-adapters**  Public archive. \\\\*\\\\* THIS REPO HAS MOVED TO  \\\\*\\\\* Adapters for integrating Model Context Protocol (MCP) tools with LangChain.js applications, supporting both stdio and SSE transports. 246 stars   34 forks   Branches   Tags   Activity. # langchain-ai/langchainjs-mcp-adapters. | Name | Name | Last commit message | Last commit date |. | tsconfig.cjs.json | tsconfig.cjs.json |  |  |. | tsconfig.examples.json | tsconfig.examples.json |  |  |. | tsconfig.tests.json | tsconfig.tests.json |  |  |. ## Repository files navigation. # LangChain.js MCP Adapters. This library provides a lightweight wrapper to allow Model Context Protocol (MCP) services to be used with LangChain.js. \\\\*\\\\* THIS REPO HAS MOVED TO  \\\\*\\\\* Adapters for integrating Model Context Protocol (MCP) tools with LangChain.js applications, supporting both stdio and SSE transports. javascript   typescript   mcp   ai-tools   langchain   llm-tools   openai-functions   langchainjs   llm-agents   agent-tools   llm-integration   model-context-protocol.', 'score': 0.88705814, 'raw_content': None}, {'url': 'https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph', 'title': 'MCP Adapters for LangChain and LangGraph', 'content': '# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.', 'score': 0.86981434, 'raw_content': None}, {'url': 'https://www.npmjs.com/package/@langchain/mcp-adapters', 'title': 'langchain/mcp-adapters', 'content': 'The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances. // Whether to throw on errors if a tool fails to load (optional, default: true). // Whether to prefix tool names with the server name (optional, default: false). // Whether to throw errors if a tool fails to load (optional, default: true). When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:. Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config. For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. const tools = await client.getTools(); // Only tools from \"working-server\".', 'score': 0.8593928, 'raw_content': None}, {'url': 'https://pypi.org/project/langchain-mcp-adapters/', 'title': 'langchain-mcp-adapters', 'content': 'Make Anthropic Model Context Protocol (MCP) tools compatible with LangChain and LangGraph agents. * ðŸ› ï¸ Convert MCP tools into LangChain tools that can be used with LangGraph agents. * ðŸ“¦ A client implementation that allows you to connect to multiple MCP servers and load tools from them. Here is a simple example of using the MCP tools with a LangGraph agent. from langchain_mcp_adapters.tools import load_mcp_tools. The library also allows you to connect to multiple MCP servers and load tools from them:. from langchain_mcp_adapters.client import MultiServerMCPClient. > from langchain_mcp_adapters.tools import load_mcp_tools. from langchain_mcp_adapters.tools import load_mcp_tools. from langchain_mcp_adapters.client import MultiServerMCPClient. from langchain_mcp_adapters.client import MultiServerMCPClient. These headers are passed with every HTTP request to the MCP server. from langchain_mcp_adapters.client import MultiServerMCPClient. If you want to run a LangGraph agent that uses MCP tools in a LangGraph API server, you can use the following setup:. from langchain_mcp_adapters.client import MultiServerMCPClient. Hashes for langchain\\\\_mcp\\\\_adapters-0.2.1-py3-none-any.whl. Details for the file `langchain_mcp_adapters-0.2.1.tar.gz`. Details for the file `langchain_mcp_adapters-0.2.1-py3-none-any.whl`.', 'score': 0.8497846, 'raw_content': None}, {'url': 'https://docs.langchain.com/oss/javascript/langchain/mcp', 'title': 'Model Context Protocol (MCP) - Docs by LangChain', 'content': 'Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the `@langchain/mcp-adapters` library. import { MultiServerMCPClient } from \"@langchain/mcp-adapters\"; import { MultiServerMCPClient } from \"@langchain/mcp-adapters\"; import { ChatAnthropic } from \"@langchain/anthropic\"; import { ChatAnthropic } from \"@langchain/anthropic\";import { createAgent } from \"langchain\"; import { createAgent } from  \"langchain\"; const client = new MultiServerMCPClient({ const  client =  new  MultiServerMCPClient({  math: { math: { transport: \"stdio\", // Local subprocess communication transport:  \"stdio\", // Local subprocess communication command: \"node\", command:  \"node\", // Replace with absolute path to your math_server.js file // Replace with absolute path to your math_server.js file args: [\"/path/to/math_server.js\"], args: [\"/path/to/math_server.js\"], }, }, weather: { weather: { transport: \"http\", // HTTP-based remote server transport:  \"http\", // HTTP-based remote server // Ensure you start your weather server on port 8000 // Ensure you start your weather server on port 8000 url: \"http://localhost:8000/mcp\", url: \"http://localhost:8000/mcp\", }, },});}); const tools = await client.getTools(); const  tools =  await  client.', 'score': 0.8479807, 'raw_content': None}], 'response_time': 1.36, 'request_id': '77138cba-5c86-4808-b4db-71f061b653f5'}}}),\n",
      "              AIMessage(content=\"The `langchain-mcp-adapters` library is a package that facilitates the integration of Model Context Protocol (MCP) tools with LangChain and LangGraph applications.\\n\\nHere's a breakdown of what it does:\\n\\n*   **Connects to MCP Servers:** It allows you to connect to one or more MCP servers and load tools from them. This means you can leverage existing MCP-compatible tools without needing to manage individual MCP client instances.\\n*   **Converts MCP Tools:** The library converts MCP tools into a format that is compatible with LangChain and LangGraph agents, making them readily usable within these frameworks.\\n*   **Supports Multiple Transports:** It supports both stdio (for local subprocess communication) and SSE (Server-Sent Events) transports, as well as HTTP for remote servers.\\n*   **Seamless Integration:** It simplifies the process of integrating a growing ecosystem of MCP tool servers into LangGraph agents, enabling agents to pull from multiple MCP servers simultaneously.\\n*   **Authentication:** For secure MCP servers that require OAuth 2.0 authentication, the library provides an `authProvider` option to handle authentication without manual header management.\\n\\nEssentially, `langchain-mcp-adapters` acts as a bridge, making it easy for LangChain and LangGraph agents to utilize the capabilities offered by MCP tools and servers.\\n\\nYou can find more information and examples on:\\n*   **GitHub:** https://github.com/langchain-ai/langchainjs-mcp-adapters (Note: This repo mentions it has moved, likely to a Python equivalent or integrated elsewhere.)\\n*   **LangChain Changelog:** https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\\n*   **npm:** https://www.npmjs.com/package/@langchain/mcp-adapters\\n*   **PyPI:** https://pypi.org/project/langchain-mcp-adapters/\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019ca373-8001-7bd3-98c2-4a119b851323-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 2289, 'output_tokens': 411, 'total_tokens': 2700, 'input_token_details': {'cache_read': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847409a3",
   "metadata": {},
   "source": [
    "## Online MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b2895fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e264dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\"),\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4725cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it in IST?', additional_kwargs={}, response_metadata={}, id='989345b7-2549-49a9-8ba8-dddbf2c6caa0'),\n",
      "              AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_time', 'arguments': '{\"timezone\": \"Asia/Kolkata\"}'}}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019ca379-7850-7c43-abf3-fa802502f0f6-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'Asia/Kolkata'}, 'id': '0f28ed0e-0668-41bc-bb43-32bb77aa128c', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 252, 'output_tokens': 21, 'total_tokens': 273, 'input_token_details': {'cache_read': 0}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"Asia/Kolkata\",\\n  \"datetime\": \"2026-02-28T14:29:35+05:30\",\\n  \"day_of_week\": \"Saturday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_0134f448-611a-457b-9535-1aaa2ee7e500'}], name='get_current_time', id='d98accf7-7e1b-4b85-b636-c274bd215bbe', tool_call_id='0f28ed0e-0668-41bc-bb43-32bb77aa128c'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019ca379-83b4-7761-9cae-9671fa329dc6-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 410, 'output_tokens': 0, 'total_tokens': 410, 'input_token_details': {'cache_read': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What time is it in IST?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
